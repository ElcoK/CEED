{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7fa466c-dd10-451b-a8e0-65fbb5505fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, roc_auc_score, roc_curve, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "sys.path.append('c://projects//osm-flex/src') \n",
    "\n",
    "from rasterstats import point_query\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "472da023-320c-4265-9cd1-167bdcdb0840",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'c://data//CEED'\n",
    "input_data = os.path.join(data_path,'input_data')\n",
    "osm_path = os.path.join(data_path,'..','CIS_EU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0372d45e-a6d7-49dd-97d4-d8c169ef7f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6bb0b5f-6955-4294-b2ce-e4029bc6eb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raster_to_vector(xr_raster):\n",
    "    \"\"\"\n",
    "    Convert a raster to a vector representation.\n",
    "\n",
    "    Args:\n",
    "        xr_raster (xarray.DataArray): Input raster data as xarray.DataArray.\n",
    "\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: Vector representation of the input raster.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert xarray raster to pandas DataFrame\n",
    "    df = xr_raster.to_dataframe()\n",
    "\n",
    "    # Filter DataFrame to select rows where band_data is 1\n",
    "    df_1 = df.loc[df.band_data > 0].reset_index()\n",
    "\n",
    "    # Create a Shapely Point geometry column from x and y values\n",
    "    df_1['geometry'] = shapely.points(df_1.x.values, df_1.y.values)\n",
    "\n",
    "    # Remove unnecessary columns from the DataFrame\n",
    "    df_1 = df_1.drop(['x', 'y', 'band', 'spatial_ref'], axis=1)\n",
    "\n",
    "    # Calculate the resolution of the raster\n",
    "    resolution = xr_raster.x[1].values - xr_raster.x[0].values\n",
    "\n",
    "    # Buffer the Point geometries by half of the resolution with square caps\n",
    "    df_1.geometry = shapely.buffer(df_1.geometry, distance=resolution/2, cap_style='square').values\n",
    "\n",
    "    # Convert the DataFrame to a GeoDataFrame\n",
    "    return gpd.GeoDataFrame(df_1)      \n",
    "\n",
    "def zonal_stats(vector, raster_in):\n",
    "    \"\"\"\n",
    "    Calculate zonal statistics of a raster dataset based on a vector dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - vector_in (str): Path to the vector dataset file (in Parquet format).\n",
    "    - raster_in (str): Path to the raster dataset file (in NetCDF format).\n",
    "    \n",
    "    Returns:\n",
    "    - pandas.Series: A series containing the zonal statistics values corresponding to each centroid point in the vector dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open the raster dataset using the xarray library\n",
    "    raster = xr.open_dataset(raster_in, engine=\"rasterio\")\n",
    "    \n",
    "    # Progress bar setup for obtaining values\n",
    "    tqdm.pandas(desc='obtain values')\n",
    "    \n",
    "    # Clip the raster dataset to the bounding box of the vector dataset\n",
    "    raster_clip = raster.rio.clip_box(vector.total_bounds[0], vector.total_bounds[1], vector.total_bounds[2], vector.total_bounds[3])\n",
    "    \n",
    "    # Convert the clipped raster dataset to a vector representation\n",
    "    raster_vector = raster_to_vector(raster_clip)\n",
    "    \n",
    "    # Create a dictionary mapping each index to its corresponding band data value\n",
    "    band_data_dict = dict(zip(list(raster_vector.index), raster_vector['band_data'].values))\n",
    "    \n",
    "    # Construct an STRtree from the vector geometry values\n",
    "    tree = shapely.STRtree(raster_vector.geometry.values)\n",
    "    \n",
    "    # Apply a function to calculate zonal statistics for each centroid point in the vector dataset\n",
    "    return vector.centroid.progress_apply(lambda x: band_data_dict[tree.query(x, predicate='intersects')[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21a931b5-def9-44d8-af4a-16d7ed81c183",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_code = 'HRV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44f9d153-dcca-4ed2-a06e-119423e5d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucco_file = os.path.join(input_data,'..','coastal_bucco_exact','{}_bucco.parquet').format(country_code)\n",
    "CLC_path = os.path.join(input_data,'u2018_clc2018_v2020_20u1_raster100m','DATA','U2018_CLC2018_V2020_20u1.tif')\n",
    "slope_path = os.path.join(input_data,'eudem_slop_3035_europe.tif')\n",
    "coastal_CLC_path = os.path.join(input_data,'CZ_2018_DU004_3035_V010.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "300e3d7a-ac4a-4ca1-b54e-d5f354c57da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_osm = gpd.read_parquet(os.path.join(osm_path,'{}_cis.parquet'.format(country_code)))                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2377a579-f05e-4717-9b12-d7b21c833169",
   "metadata": {},
   "outputs": [],
   "source": [
    "roads = gpd.GeoDataFrame(country_osm.loc['road'][['geometry','highway','maxspeed','lanes','surface']])\n",
    "roads = roads.to_crs(3035)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abde56b8-dead-4c3b-88b2-fd0c7ef50412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "obtain values: 100%|████████████████████████████████████████████████████████| 152059/152059 [00:05<00:00, 29105.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4min 5s\n",
      "Wall time: 4min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "land_use = zonal_stats(roads,CLC_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf4ef7b-1ecc-46c8-b6b4-304bbead8f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "slope = zonal_stats(roads,slope_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561e71fb-5d76-4819-a477-4ec5424152ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "roads['landuse'] =  land_use\n",
    "roads['slope'] =  slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583f838d-f261-435d-9071-6b8465a56259",
   "metadata": {},
   "outputs": [],
   "source": [
    "geom = roads.iloc[-1].geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bb25b3-4858-48e4-b50f-b50e5523d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinuosity(geom):\n",
    "    if geom.geom_type == 'MultiPolygon':\n",
    "        return 1\n",
    "    elif geom.geom_type == 'LineString':      \n",
    "        return shapely.length(geom)/shapely.distance(shapely.get_point(geom,0),shapely.get_point(geom,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f3a7b1-22c7-47c4-8b27-d99076780654",
   "metadata": {},
   "outputs": [],
   "source": [
    "roads['sinuosity'] = roads.geometry.progress_apply(lambda x: sinuosity(x))\n",
    "roads.sinuosity.loc[roads.sinuosity>roads.sinuosity.quantile(.99)] = roads.sinuosity.quantile(.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0055e24-fa39-4ca3-8510-1640d5589971",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = roads.dropna()\n",
    "\n",
    "full_data.loc[full_data['surface'].map(full_data['surface'].value_counts(normalize=True)\n",
    "                                      .lt(0.001)),'surface'] = 'other'\n",
    "\n",
    "full_data.loc[full_data['highway'].map(full_data['highway'].value_counts(normalize=True)\n",
    "                                      .lt(0.01)),'highway'] = 'other'\n",
    "\n",
    "full_data.loc[full_data['lanes'].map(full_data['lanes'].value_counts(normalize=True)\n",
    "                                      .lt(0.01)),'lanes'] = 'other'\n",
    "\n",
    "full_data.loc[full_data['landuse'].map(full_data['landuse'].value_counts(normalize=True)\n",
    "                                      .lt(0.01)),'landuse'] = 0\n",
    "\n",
    "full_data.landuse = full_data.landuse.astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b37d0b3-226a-4e7a-9989-7dd4bcea242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def develop_predictor(data,y_col='maxspeed',x_cols=['land_use','highway','lanes','surface']):\n",
    "\n",
    "    y = data[y_col]\n",
    "    X = data[x_cols]\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.66)\n",
    "    \n",
    "    features_to_encode = X_train.columns[X_train.dtypes==object].tolist()  \n",
    "       \n",
    "    col_trans = make_column_transformer(\n",
    "                        (OneHotEncoder(),features_to_encode),\n",
    "                        remainder = \"passthrough\"\n",
    "                        )\n",
    "    \n",
    "    rf_classifier = RandomForestClassifier(\n",
    "                      criterion='gini',\n",
    "                      min_samples_leaf=25,\n",
    "                      n_estimators=50,\n",
    "                      bootstrap=True,\n",
    "                      oob_score=True,\n",
    "                      n_jobs=-1)\n",
    "    \n",
    "#     # Number of trees in random forest\n",
    "#     n_estimators = [int(x) for x in np.linspace(start = 50, stop = 1000, num = 50)]\n",
    "#     # Number of features to consider at every split\n",
    "#     max_features = ['log2', 'sqrt']\n",
    "#     # Maximum number of levels in tree\n",
    "#     max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "#     max_depth.append(None)\n",
    "#     # Minimum number of samples required to split a node\n",
    "#     min_samples_split = [2, 5, 10]\n",
    "#     # Minimum number of samples required at each leaf node\n",
    "#     min_samples_leaf = [1, 2, 4]\n",
    "#     # Method of selecting samples for training each tree\n",
    "#     bootstrap = [True, False]\n",
    "#     # Create the random grid\n",
    "#     random_grid = {'n_estimators': n_estimators,\n",
    "#                    'max_features': max_features,\n",
    "#                    'max_depth': max_depth,\n",
    "#                    'min_samples_split': min_samples_split,\n",
    "#                    'min_samples_leaf': min_samples_leaf,\n",
    "#                    'bootstrap': bootstrap}\n",
    "\n",
    "#     rf_random = RandomizedSearchCV(estimator = rf_classifier, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "    \n",
    "    # Fit the random search model\n",
    "    #rf_random.fit(train_features, train_labels)\n",
    "\n",
    "    \n",
    "    pipe = make_pipeline(col_trans, rf_classifier)\n",
    "    \n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = pipe.predict(X_test)\n",
    "    \n",
    "    accuracy_score(y_test, y_pred)\n",
    "    print(f\"The accuracy of the model is {round(accuracy_score(y_test,y_pred),3)*100} %\")\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9ee350-943f-490b-8970-083794554623",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipe = develop_predictor(full_data,y_col='lanes',x_cols=['landuse','highway','sinuosity','slope'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91eab925-e0db-4df5-9fd9-318785586d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def develop_ann(data,y_col='maxspeed',x_cols=['land_use','highway','lanes','surface']):\n",
    "\n",
    "    y = data[y_col]\n",
    "    X = data[x_cols]\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.333)\n",
    "    \n",
    "    features_to_encode = X_train.columns[X_train.dtypes==object].tolist()  \n",
    "       \n",
    "    col_trans = make_column_transformer(\n",
    "                        (OneHotEncoder(),features_to_encode),\n",
    "                        remainder = \"passthrough\"\n",
    "                        )\n",
    "    \n",
    "    clf = MLPClassifier(solver='adam', alpha=1e-5,activation='relu',learning_rate_init=0.001,\n",
    "                     hidden_layer_sizes=(50, 4), random_state=1,max_iter=1000)\n",
    "\n",
    "    pipe = make_pipeline(col_trans, clf)\n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = pipe.predict(X_test)\n",
    "    \n",
    "    accuracy_score(y_test, y_pred)\n",
    "    print(f\"The accuracy of the model is {round(accuracy_score(y_test,y_pred),3)*100} %\")\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d556e46-71ed-420d-a31b-8c655a961548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is 43.3 %\n",
      "CPU times: total: 11.9 s\n",
      "Wall time: 12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipe_nn = develop_ann(full_data,y_col='maxspeed',x_cols=['landuse','highway','sinuosity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755b5646-5b9a-4de1-a1ee-27d84b2fda31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
